# **Kafka to MongoDB Stream â€“ Logistics Data**

Welcome to the Kafka to MongoDB Stream Project repository! 
This project demonstrates a real-time data streaming pipeline for logistics data using Kafka, Schema Registry, Docker, and MongoDB. It highlights important data engineering practices such as data ingestion, schema enforcement, deduplication, cleansing, and scalable consumer groups.

â¸»

# **ğŸ—ï¸ Data Architecture**

**The architecture is designed for real-time logistics data streaming:**

	1.	CSV Source â€“ delivery_trip_truck_data.csv serves as the raw logistics dataset.
	2.	Kafka Producer â€“ Reads data from CSV and publishes messages to a Kafka topic.
	3.	Schema Registry & Clustering â€“ Ensures message schema consistency and scalable Kafka deployment.
	4.	Kafka Consumers (via Docker) â€“ A consumer group with multiple instances applies filtration and cleansing rules before storing the data.
	5.	MongoDB (LogisticsDB) â€“ Final storage, containing the truck_data collection, with deduplication (no duplicate IDs).
	6.	Confluent Sink Connector â€“ Used for Kafka â†’ MongoDB integration from Confluent Kafka UI.

# **Project Overview**

**This project involves:**

	1.	Producer Code â€“ Reads from the logistics CSV and pushes records into Kafka.
	2.	Schema Registry Setup â€“ Manages message schemas for consistency.
	3.	Consumer Code â€“ Applies data cleansing, filtering, and deduplication before writing to MongoDB.
	4.	MongoDB Database â€“ Stores clean data under logisticsDB â†’ truck_data collection.
	5.	Dockerized Consumers â€“ Run inside Docker containers with Dockerfile and docker-compose.
	6.	Consumer Group Scaling â€“ Supports multiple consumer instances (e.g., 5 parallel consumers).
	7.	Confluent Sink Connector â€“ Connects Kafka with MongoDB for ingestion.

**This repository is an excellent showcase of:** 

	â€¢	Kafka â†’ MongoDB streaming pipelines
	â€¢	Data deduplication using unique IDs
	â€¢	Schema management with Confluent Schema Registry
	â€¢	Scalable consumer groups running in Docker
	â€¢	End-to-end logistics data ingestion workflows

â¸»

# **Features**

	â€¢	CSV â†’ Kafka â†’ MongoDB streaming flow
	â€¢	Schema Registry for schema enforcement
	â€¢	Data cleansing & filtering in consumer code
	â€¢	No duplicate IDs in MongoDB collection
	â€¢	Dockerized consumers for easy deployment
	â€¢	Scalable consumer group (run multiple instances dynamically)
	â€¢	Confluent Sink Connector for seamless Kafkaâ€“MongoDB integration

â¸»

# **Tech Stack**

	â€¢	Confluent Kafka â€“ Distributed event streaming platform
	â€¢	Confluent Schema Registry â€“ Enforces schema consistency
	â€¢	MongoDB â€“ NoSQL database for logistics data storage
	â€¢	Python â€“ Producer & consumer implementation
	â€¢	Docker & Docker Compose â€“ Containerized consumer deployment
	â€¢	Confluent Kafka UI â€“ For sink connector and monitoring

â¸»

# **Project Workflow**

**Objective** - Build a real-time logistics data pipeline that ingests trip & truck data from CSV into MongoDB, ensuring cleansing, deduplication, and scalability.

**Workflow Steps**

	1.	Producer â€“ Publishes logistics data (delivery_trip_truck_data.csv) into Kafka.
	2.	Schema Registry & Cluster â€“ Enforces schema validation and handles scaling.
	3.	Consumer Group (via Docker) â€“ Multiple consumer instances run in parallel to consume, filter, and cleanse data.
	4.	Deduplication â€“ Consumer logic ensures no duplicate truck IDs are inserted into MongoDB.
	5.	MongoDB Storage â€“ Cleaned data is stored inside logisticsDB â†’ truck_data.
	6.	Confluent Sink Connector â€“ Connects Kafka topic to MongoDB for automated ingestion.

â¸»

ğŸ“‚ Repository Structure

```
kafka-to-mongodb-stream/
â”œâ”€â”€ datasets/                   # Source dataset (delivery_trip_truck_data.csv)
â”‚
â”œâ”€â”€ Producer-Cosumer/
|	â”œâ”€â”€ produce.py          #Producer code (CSV Data - Kafka Topic)
â”‚   â””â”€â”€ Consumer-Group
|       â”œâ”€â”€ consumer.py        #Consumer code (Kafka -> MongoDB With Cleansing and Filteration)
â”‚   	â””â”€â”€ docker file
|		â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ README.md               # Project documentation
â”œâ”€â”€ LICENSE                 # License information
```

# **Key Learning Outcomes**

	â€¢	How to build a real-time data pipeline with Kafka & MongoDB
	â€¢	Using Schema Registry to enforce data quality
	â€¢	Implementing cleansing & deduplication at the consumer level
	â€¢	Running scalable consumers in Docker containers
	â€¢	Using Confluent Sink Connector for database integration

â¸»

# **ğŸ›¡ï¸ License**

This project is licensed under the MIT License.

â¸»

# **ğŸŒŸ About Me**

Hi there! Iâ€™m Krrish Sethiya. Iâ€™m a 3rd Year Grad at Medicaps University, Indore, currently specializing in Data Engineering.
